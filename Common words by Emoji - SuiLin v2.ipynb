{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML, display\n",
    "HTML(\"<style>.container { width:100% !important; }</style>\")\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "from Data.sentiment_dict import positive, negative\n",
    "#from sklearn.feature_extraction import DictVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "#pd.options.display.max_colwidth=250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# separate tweet into parts with emojis of interest and the rest\n",
    "def contain_emoji(tweet, emojis):\n",
    "    tweet_emoji = set(tweet).intersection(emojis)\n",
    "    if len(tweet_emoji) == 0:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def filter_emoji(tweet, emojis):\n",
    "    # Return the text without emojis.\n",
    "    text = \"\"\n",
    "    for i in tweet:\n",
    "        if i not in emojis:\n",
    "            text += i\n",
    "    return text\n",
    "\n",
    "def filter_stop(word_list, stop_words):\n",
    "    # Return the words that are not contained in 'stop_words'.\n",
    "    return [word for word in word_list if word.lower() not in stop_words]\n",
    "\n",
    "def convert_to_ngrams(text, n):    # num_ngrams = number of ngrams given user-defined n  e.g. n=1 unigrams; n=2 bigrams\n",
    "    # Convert text to a list of words.\n",
    "    word_list = text.split()\n",
    "    num_ngrams=len(word_list) - n + 1\n",
    "    ngram_list = []\n",
    "    # i = location of ngram's starting word in word list / tweet\n",
    "    for i in range (0, num_ngrams):\n",
    "        # Construct the ngram by chaining the words together.\n",
    "        ngram = word_list[i]\n",
    "        # j is the location of word in the ngram.\n",
    "        for j in range (1, n):\n",
    "            ngram += \" \" + word_list[i + j]\n",
    "        ngram_list.append(ngram)\n",
    "    return ngram_list\n",
    "\n",
    "def update_ngram_count(text, stop_words, n, ngram_count):\n",
    "    # Update 'ngram_count' dictionary with the frequency ngrams that present in 'text'. 'n' is the\n",
    "    # degree of ngrams. If 'n' is 1, then we will filter the unigram using 'stop_words'.\n",
    "    ngram_list = convert_to_ngrams(text, n)\n",
    "    # Only filter stop words for unigrams i.e. ngram_n=1\n",
    "    if n == 1:\n",
    "        ngram_list = filter_stop(ngram_list, stop_words)\n",
    "    for ngram in set(ngram_list):\n",
    "        if ngram not in ngram_count:\n",
    "            ngram_count[ngram] = 1\n",
    "        else: \n",
    "            ngram_count[ngram] += 1\n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# specify the location of the tweets\n",
    "negtweet_file ='./Data/negtweets.txt'\n",
    "postweet_file ='./Data/postweets.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Top 50 Positive Ngrams, where n=', 4)\n",
      "(u'I love you so', 70)\n",
      "(u'love you so much', 52)\n",
      "(u\"watch 's new video\", 29)\n",
      "(u\"Go watch 's new\", 23)\n",
      "(u\"can't wait to see\", 18)\n",
      "(u'I love you too', 17)\n",
      "(u\"go watch 's new\", 17)\n",
      "(u\"I can't wait to\", 14)\n",
      "(u'IT A LIKE &amp;', 14)\n",
      "(u'i love you so', 13)\n",
      "(u\"WATCH 's NEW VIDEO\", 12)\n",
      "(u\"You Don't Have To\", 11)\n",
      "(u\"Don't Have To Go\", 11)\n",
      "(u'Can you follow me', 11)\n",
      "(u'GIVE IT A LIKE', 11)\n",
      "(u'love you so so', 10)\n",
      "(u\"I'm so happy for\", 10)\n",
      "(u'it that night\\U0001f525\\U0001f525 Everybody', 10)\n",
      "(u'night\\U0001f525\\U0001f525 Everybody $1 til', 10)\n",
      "(u'SPIKED it that night\\U0001f525\\U0001f525', 10)\n",
      "(u'video \\u25b6 [ ]', 10)\n",
      "(u'will find out who', 10)\n",
      "(u'who SPIKED it that', 10)\n",
      "(u'#WhoSpikedThePunch is Back Feb.', 10)\n",
      "(u'Everybody $1 til 11:30', 10)\n",
      "(u'that night\\U0001f525\\U0001f525 Everybody $1', 10)\n",
      "(u'find out who SPIKED', 10)\n",
      "(u'Have To Go by', 10)\n",
      "(u'I LOVE YOU SO', 10)\n",
      "(u'out who SPIKED it', 10)\n",
      "(u\"Y'all will find out\", 10)\n",
      "(u\"I'm so proud of\", 9)\n",
      "(u\"'s New video! IS\", 9)\n",
      "(u'#FaceTimeMeNash BRAZIL LOVES YOU', 9)\n",
      "(u'please follow my IG!', 9)\n",
      "(u'IS PERFECT \\U0001f3a5\\U0001f39e \\U0001f449\\U0001f3fc', 9)\n",
      "(u\"go watch the 's\", 9)\n",
      "(u'PERFECT \\U0001f3a5\\U0001f39e \\U0001f449\\U0001f3fc #FaceTimeMeNash', 9)\n",
      "(u'New video! IS PERFECT', 9)\n",
      "(u'\\U0001f3a5\\U0001f39e \\U0001f449\\U0001f3fc #FaceTimeMeNash BRAZIL', 9)\n",
      "(u'thank you so much', 9)\n",
      "(u\"the 's New video!\", 9)\n",
      "(u\"watch the 's New\", 9)\n",
      "(u'video! IS PERFECT \\U0001f3a5\\U0001f39e', 9)\n",
      "(u'Hello please follow my', 9)\n",
      "(u'\\U0001f449\\U0001f3fc #FaceTimeMeNash BRAZIL LOVES', 9)\n",
      "(u'Everyone go watch the', 9)\n",
      "(u'follow my IG! ILY!', 9)\n",
      "(u'makes me so happy', 9)\n",
      "(u'Hey! Go watch New', 8)\n",
      "\n",
      "Top 50 Negative Ngrams, where n=4\n",
      "(u'I love you so', 33)\n",
      "(u'I just want to', 30)\n",
      "(u\"I don't want to\", 28)\n",
      "(u'love you so much', 27)\n",
      "(u'I want to go', 25)\n",
      "(u'I wish I could', 25)\n",
      "(u'want to go to', 21)\n",
      "(u'I wish I was', 19)\n",
      "(u'to go to the', 18)\n",
      "(u'I wish I had', 18)\n",
      "(u\"I don't know what\", 18)\n",
      "(u'I miss you too', 17)\n",
      "(u'miss you so much', 16)\n",
      "(u'I miss you so', 15)\n",
      "(u'know what to do', 15)\n",
      "(u'I need to get', 14)\n",
      "(u'to go back to', 13)\n",
      "(u'I feel like I', 13)\n",
      "(u'be the death of', 13)\n",
      "(u'I really want to', 13)\n",
      "(u'to go to sleep', 13)\n",
      "(u'I need to go', 13)\n",
      "(u'Dropped (Tmilli x Stand', 12)\n",
      "(u'For Something) \\u270a\\U0001f525 Click', 12)\n",
      "(u'Click The Link \\U0001f447\\U0001f447\\U0001f447', 12)\n",
      "(u'Something) \\u270a\\U0001f525 Click The', 12)\n",
      "(u\"I can't wait to\", 12)\n",
      "(u'Stand For Something) \\u270a\\U0001f525', 12)\n",
      "(u'Just Dropped (Tmilli x', 12)\n",
      "(u'x Stand For Something)', 12)\n",
      "(u'New Video Just Dropped', 12)\n",
      "(u'Video Just Dropped (Tmilli', 12)\n",
      "(u'\\u270a\\U0001f525 Click The Link', 12)\n",
      "(u'(Tmilli x Stand For', 12)\n",
      "(u\"don't know what to\", 12)\n",
      "(u\"\\U0001f497. I'm so sorry\", 11)\n",
      "(u'for spam . .', 11)\n",
      "(u'I need a new', 11)\n",
      "(u\"I feel like I'm\", 11)\n",
      "(u'I wanna go to', 11)\n",
      "(u'so sorry for spam', 11)\n",
      "(u'\\U0001f496. \\U0001f49d. \\U0001f498. \\U0001f497.', 11)\n",
      "(u\"can't wait to get\", 11)\n",
      "(u'sorry for spam .', 11)\n",
      "(u\"\\U0001f498. \\U0001f497. I'm so\", 11)\n",
      "(u'Hi \\U0001f64a. \\U0001f496. \\U0001f49d.', 11)\n",
      "(u\"I'm so sorry for\", 11)\n",
      "(u\"\\U0001f49d. \\U0001f498. \\U0001f497. I'm\", 11)\n",
      "(u'at the same time', 11)\n",
      "(u'I really need to', 11)\n"
     ]
    }
   ],
   "source": [
    "# The words in the tweets that contain positive emoji and their count. We only count the word once even if the word appears\n",
    "# multiple times in a tweet.\n",
    "positive_ngram_count = {}\n",
    "# The words in the tweets that contain positive emoji and their count. We only count the word once even if the word appears\n",
    "# multiple times in a tweet.\n",
    "negative_ngram_count = {}\n",
    "\n",
    "all_emojis = positive + negative\n",
    "#stop_words=stopwords.words(\"english\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "added_stop_words = [\"i'm\", \"it's\"]\n",
    "stop_words.update(added_stop_words)\n",
    "\n",
    "ii = 0\n",
    "ngram_n = 4\n",
    "# load all the tweets from negative file\n",
    "with open(postweet_file,'r') as f:\n",
    "    for line in f:\n",
    "        # For testing purpose, only process a limited number of tweets in the file.\n",
    "        #if ii >= 200:\n",
    "         # break\n",
    "        #ii = ii +1\n",
    "        \n",
    "        tweet = line.decode('utf-8')\n",
    "        text_without_emoji = filter_emoji(tweet, all_emojis)\n",
    "        # Process the tweet that contains positive emoji.\n",
    "        if contain_emoji(tweet, positive):\n",
    "            update_ngram_count(text_without_emoji, stop_words, ngram_n, \n",
    "                               positive_ngram_count)\n",
    "                    \n",
    "        # Process the tweet that contains negative emoji.            \n",
    "        if contain_emoji(tweet, negative):\n",
    "            update_ngram_count(text_without_emoji, stop_words, ngram_n, \n",
    "                               negative_ngram_count)\n",
    "\n",
    "# load all the tweets from negative file\n",
    "ii = 0\n",
    "with open(negtweet_file,'r') as f:\n",
    "    for line in f:\n",
    "        # For testing purpose, only process a limited number of tweets in the file.\n",
    "        # if ii >= 200:\n",
    "        #    break\n",
    "        #ii = ii +1\n",
    "        \n",
    "        tweet = line.decode('utf-8')\n",
    "        text_without_emoji = filter_emoji(tweet, all_emojis)\n",
    "        \n",
    "         # Process the tweet that contains positive emoji.\n",
    "        if contain_emoji(tweet, positive):\n",
    "            update_ngram_count(text_without_emoji, stop_words, ngram_n, \n",
    "                               positive_ngram_count)\n",
    "                    \n",
    "        # Process the tweet that contains negative emoji.            \n",
    "        if contain_emoji(tweet, negative):\n",
    "            update_ngram_count(text_without_emoji, stop_words, ngram_n, \n",
    "                               negative_ngram_count)\n",
    "    \n",
    "# Sort the positive and negative words based on their counts.\n",
    "# sorted in descending order using \"reverse = true\"\n",
    "sorted_positive_ngrams = sorted(positive_ngram_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "#print \"\\nSorted Positive Word Count\"\n",
    "#print sorted_positive_words\n",
    "print (\"Top 50 Positive Ngrams, where n=\", ngram_n)\n",
    "ii = 0\n",
    "for ngram in sorted_positive_ngrams: \n",
    "    if ii >= 50:\n",
    "        break\n",
    "    ii +=1\n",
    "    print ngram\n",
    "\n",
    "sorted_negative_ngrams = sorted(negative_ngram_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print (\"\\nTop 50 Negative Ngrams, where n=%d\" % ngram_n)\n",
    "ii = 0\n",
    "for ngram in sorted_negative_ngrams: \n",
    "    if ii >= 50:\n",
    "        break\n",
    "    ii +=1\n",
    "    print ngram\n",
    "\n",
    "#normalize by odds ratio\n",
    "#use mike's new data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
